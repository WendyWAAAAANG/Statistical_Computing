---
title: "Homework Assignment 3"
author: "DS4043, Spring 2023"
date: "__Due on April 14, 2023 at 11:59 pm__"
output: pdf_document
---


```{r setup, include=FALSE}
library(knitr)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5)
options(digits = 4)


## indents are for indenting r code as formatted text
## They may need to be adjusted depending on your OS
# if your output looks odd, increase or decrease indent
indent1 = '    '
indent2 = '        '
indent3 = '            '
#library(tidyverse)
#library(reshape2)
```


1. The $\operatorname{Pareto}(a, b)$ distribution has cdf
$$
F(x)=1-\left(\frac{b}{x}\right)^{a}, \quad x \geq b>0, a>0
$$

    (a) Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample with size 1000 from the Pareto $(2,2)$ distribution.

  Let u equals to F(x) to find inverse function.
$$
u=1-\left(\frac{b}{x}\right)^{a}
$$
  And we have:
$$
x=\frac{b}{\sqrt[a]{1-u}}
$$
  According to domain of F(x), we can find its value between (0, 1), which is the domain of inverse function.

  Therefore, the reverse function is:
$$
F^{-1}(x)=\frac{b}{\sqrt[a]{1-x}},\quad 1\geq x \geq b>0, \quad a,b>0
$$


    (b) Graph the density histogram of the sample with the Pareto $(2,2)$ density superimposed for comparison.

```{r}
n <- 1000
u <- runif(n)
x <- 2/sqrt(1-u)

hist(x, freq = FALSE, xlab = "x", main = "Density Function with Pareto (2,2)")
curve(8/(x)^3, add = TRUE, lwd = 2)
lines(density(x), lty = 2, lwd = 2)
```


2. A discrete random variable $X$ has probability mass function
\begin{tabular}{cccccc}
$x$ & 0 & 1 & 2 & 3 & 4 \\
\hline $p(x)$ & $0.1$ & $0.2$ & $0.2$ & $0.2$ & $0.3$
\end{tabular}
    (a) Use the inverse transform method to generate a random sample of size 1000 from the distribution of $X$. (Hint: search _if else if_ function)

```{r}
n <- 1000
p <- runif(n)
x <- numeric(n)
for (i in 1:n){
  if(p[i]<0.1){
    x[i] = 0
  }
  else if(p[i]<0.3 & p[i]>0.1){
    x[i] = 1
  }
  else if(p[i]<0.5 & p[i]>0.3){
    x[i] = 2
  }
  else if(p[i]<0.7 & p[i]>0.5){
    x[i] = 3
  }
  else{
    x[i] = 4
  }
}

head(x,10)
```

    (b) Construct a relative frequency table and compare the empirical with the theoretical probabilities

```{r}
# empirical probabilities:
q <- seq(.1,.8,.1)
Empirical <- quantile(x, q)

# theoretical probabilities:
n <- 1000
u <- numeric(n)
for (i in 1:n){
  if(i <= 1000*0.1){
    u[i] = 0
  }
  else if(i > 1000*0.1 & i <= 1000*0.3){
    u[i] = 1
  }
  else if(i > 1000*0.3 & i <= 1000*0.5){
    u[i] = 2
  }
  else if(i > 1000*0.5 & i <= 1000*0.7){
    u[i] = 3
  }
  else{
    u[i] = 4
  }
}
Theoretical <- quantile(u, q)
round(rbind(Empirical, Theoretical), 3)
```

    (c) Repeat b) by using $R$ function 'sample' to generate a random sample of $X$ with sample size 1000, and then show the relative frequency table.
    
```{r}
y <- sample(x, 1000, replace = TRUE)
table(y)
```


3. Consider the integration $\theta=\int_{A} g(x) d x$, where
$$
g(x)=x e^{-\frac{x^{2}}{2}}
$$
and $A \in\{x: 1<x<\infty\}$, the importance function
$$
f(x)=e^{-(x-1)},
$$
for $1<x<\infty$, compute the Monte Carlo estimator $\hat{\theta}$ and $\operatorname{Var}(\hat{\theta})$ using the importance sampling method.

```{r}
m <- 1000
g <- function(x){
  x*exp(-x^2/2)
}
u <- runif(m)
x <- -log(-u+1)+1
fg <- g(x) / (exp(-(x-1)))
theta.hat <- mean(fg)
se <- sd(fg)
round(rbind(theta.hat, se), 3)
```


4. (a) Find two importance functions $f_{1}$ and $f_{2}$ that are supported on $(1, \infty)$ and are 'close' to
$$
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1 .
$$
Which of your two importance functions should produce the smaller variance in estimating
$$
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} d x
$$
by importance sampling? Explain.

 Firstly, we plot curve of g(x), so that we can inuitively think of which function is close to g(x).
```{r}
x <- seq(1,10,.01)
g <- function(x){
  (x^2/sqrt(2*pi))*exp(-x^2/2)
}
plot(g(x), type = 'l')
```

  Here, I choose Chi-square function with degree of freedom 4 as $f_{1} = Gamma(2,2)$ and function in question3 as $f_{2}=e^{-(x-1)}$.

    (b) Obtain the Monte Carlo estimates of the previous integral using those two importance functions $f_{1}$ and $f_{2}$. Also calculate their corresponding estimator variance. Which importance function produces larger variance? Please explain. Let the number of simulations m = 10000. **(Note: the integral is from 1 to $\infty$)**

```{r}
m <- 10000
theta.hat <- se <- variance <- numeric(2)
g <- function(x){
  (x^2/sqrt(2*pi))*exp(-x^2/2)*(x>1)
}

# using f1.
x <- rgamma(m, shape=2, scale=2)
# to catch overflow errors in g(x)
i <- c(which(x>1))
x[i] <- 2
fg <- g(x) / dgamma(x, shape=2, scale=2)
theta.hat[1] <- mean(fg)
se[1] <- sd(fg)
variance[1] <- var(fg)

# using f2.
u <- runif(m)
x <- -log(-u+1)+1
fg <- g(x) / (exp(-(x-1)))
theta.hat[2] <- mean(fg)
se[2] <- sd(fg)
variance[2] <- var(fg)

round(rbind(theta.hat,se,variance),4)
```

5. A certain type of electronic component has a lifetime $Y$ (in hours) with probability density function given by
$$
f(y \mid \theta)= \begin{cases}\left(\frac{1}{\theta^{2}}\right) y e^{-y / \theta}, & y>0 \\ 0, & \text { otherwise. }\end{cases}
$$
That is, $Y$ has a gamma distribution with parameters $\alpha=2$ and $\theta$. Let $\hat{\theta}$ denote the MLE of $\theta$. Suppose that $n$ such components, tested independently, had lifetimes of $Y_1, Y_2, \ldots, Y_n$ hours. Find the MLE of $\theta$.

Here we derive likelihood function of Y:
$$
L(\theta)=\prod f(y_1,y_2,\ldots,y_n|\theta)=f(y_1|\theta)*f(y_2|\theta)*\ldots*f(y_n|\theta)\\=\prod\frac{1}{\theta^{2n}} y_ie^{\frac{-\sum{y_i}}{\theta}}
$$

Take log of likelihood function:
$$
l(\theta)=\log(L(\theta))\\=\sum{log(f(x_i|\theta))}\\=\log{\frac{1}{\theta^{2n}}}+\log{\pi{y_i}}+\log{e}^{-\frac{\sum{y_i}}{\theta}}\\=-2n\log{\theta}+\log{\pi{y_i}}-\frac{\sum{y_i}}{\theta}
$$

  Let its derivative equals to 0, we can have equation below:
$$
\frac{\partial{l(y)}}{\partial{\theta}}=\frac{-2n}{\theta}+\frac{\sum{y_i}}{\theta^2}=0
$$
 Finally calculate the MLE of $\theta$:
$$
\sum_{i=1}^{n}{y_i}=2n\theta\\\hat\theta=\frac{\hat{Y}}{2}
$$




    First, we should calculate the mle of x by hand:
    Here we derive likelihood function of Y:
$$
L(\theta)=\prod f(y_1,y_2,\ldots,y_n|\theta)=f(y_1|\theta)*f(y_2|\theta)*\ldots*f(y_n|\theta)\\=\prod\frac{1}{\lambda} e^{-\frac{x}{\lambda}}
$$
    Take log of likelihood function:
$$
l(\theta)=\log(L(\theta))\\=\sum{log(f(x_i|\theta))}\\=\log{\frac{1}{\theta^{2n}}}+\log{\pi{y_i}}+\log{e}^{-\frac{\sum{y_i}}{\theta}}\\=-2n\log{\theta}+\log{\pi{y_i}}-\frac{\sum{y_i}}{\theta}
$$
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
