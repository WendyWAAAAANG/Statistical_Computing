---
title: "Homework Assignment 4"
author: "DS4043 Spring 2023"
date: "__Due on April 28, 2023 at 11:59 pm__"
output: pdf_document
---


```{r setup, include=FALSE}
library(knitr)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5)
options(digits = 4)


## indents are for indenting r code as formatted text
## They may need to be adjusted depending on your OS
# if your output looks odd, increase or decrease indent
indent1 = '    '
indent2 = '        '
indent3 = '            '
#library(tidyverse)
#library(reshape2)
```


1. Consider random variables $X_{1}, \ldots, X_{n}$ are i.i.d. $\mathrm{N}\left(\mu=30, \sigma^{2}=100\right)$, given $n=50$ and $\alpha=0.05$.
    a) Obtain the **Monte Carlo estimate of the confidence level** for the $95 \%$ confidence interval includes the true value of $\mu$. Let the number of replicate as $m = 1000$. (Hint: you need to construct a 95% confidence interval of $\mu$; the statistic is the sample mean.)

        <!-- solution begin -->
        __Solution__:
        
```{r indent = indent2}
m = 1000; n = 50; mu <- 30; sigma_2 <- 100
set.seed(15)

calcCI <- function(n, alpha) {
 y <- rnorm(n, mean = mu, sd = sqrt(sigma_2))
 return((n-1) * var(y) / qchisq(alpha, df = n-1))
}
UCL <- replicate(m, expr=calcCI(n = n, alpha=.05))

confidence_level <- mean(UCL > 100)
cat("Monte Carlo estimate of confidence level:", confidence_level, "\n")
```

        <!-- solution end -->    
    
    b) For the hypotheses, $H_{0}: \mu=30$ vs $H_{1}: \mu \neq 30$, use Monte Carlo method to compute an empirical probability of type-I error, and compare it with the true value. Let the number of replicate as $m = 10000$.
  
        <!-- solution begin -->
        __Solution__:
        
```{r indent = indent2}
m = 10000; n = 50; mu <- 30
set.seed(13)

alpha <- .05; sigma <- 100
p <- numeric(m) #storage for p-values

for (j in 1:m) {
  x <- rnorm(n, mu, sigma)
  ttest <- t.test(x, alternative = "two.sided", mu = mu)
  p[j] <- ttest$p.value
}
p.hat <- mean(p < alpha)
# Print the result
cat("Empirical probability of type-I error:", p.hat, "\n")
cat("True value of type-I error:", alpha, "\n")
```
    
        <!-- solution end -->
        
  
        
2. Consider the random variables $X_{1}, \ldots, X_{n}$ are i.i.d. with a mixture normal density, i.e.
$$
(1-p) N\left(\mu=0, \sigma^{2}=1\right)+p N\left(\mu=1, \sigma^{2}=9\right)
$$
We have $\alpha=0.05, p=0.4$ and $n=50$. Let $\beta_{1}$ denote the skewness of random variable $X$ and its sample estimate is denoted by $b_{1}$. The hypotheses are $H_{0}: \beta_{1}=0$ vs $H_{1}: \beta_{1} \neq 0$. Use the Monte Carlo method to estimate **empirical power** of the hypotheses. 
For finite samples one should use
$$
\operatorname{Var}\left(b_{1}\right)=\frac{6(n-2)}{(n+1)(n+3)}.
$$
Let the number of replicate as $m = 10000$.
To generate number from mixture density. Suppose $X_{1} \sim N(0,1)$ and $X_{2} \sim N(3,1)$ are independent.
We can define a $50 \%$ normal mixture $X$, denoted $F_{X}(x)=0.5 F_{X_{1}}(x)+$ $0.5 F_{X_{2}}(x)$. Unlike the convolution, the distribution of the mixture $X$ is distinctly non-normal; it is bimodal.
To simulate the mixture:
    1. Generate an integer $k \in\{1,2\}$, where $P(1)=P(2)=0.5$.
    2. If $k=1$ deliver random $x$ from $\mathrm{N}(0,1)$; if $k=2$ deliver random $x$ from $\mathrm{N}(3,1)$.

    <!-- solution begin -->
    __Solution__:
```{r indent=indent1}
set.seed(19)
alpha <- 0.05; p <- 0.4; n <- 50; m <- 10000

cv <- qnorm(.975, 0, sqrt(6*(n-2)/((n+1)*(n+3))))
sk <- function(x){
  xbar <- mean(x)
  m3 <- mean((x - xbar)^3)
  m2 <- mean((x - xbar)^2)
  return( m3 / m2^1.5 ) 
}

skewness.cl = function(n, cv){
  #p.reject <- numeric(length(n))  #to store sim. results
  
  sktests <- numeric(m) #test decisions
  for (i in 1:m){
    # generate x with mixture distributions.
    x1 <- rnorm(n, mean = 0, sd = 1)
    x2 <- rnorm(n, mean = 1, sd = 3)
    #s <- x1 + x2 #the convolution
    u <- runif(n)
    k <- as.integer(u > 0.4) #vector of 0s and 1s
    x <- k * x2 + (1-k) * x1 #the mixture
      
    # test decision is 1 (reject) or 0
    sktests[i] <- as.integer(abs(sk(x)) >= cv)
  }
  # get the proportion rejected
  p.reject <- mean(sktests) # proportion rejected
  
  return(p.reject)
}

epr_pow <- skewness.cl(n, cv)
cat("Empirical power of the hypotheses: ", epr_pow)
```

    <!-- solution end -->
        
3. Compute a jackknife estimate of the bias and the standard error of the correlation statistic in the _law_ data example. Compare the result with the bootstrap method.

    <!-- solution begin -->
    __Solution__:
```{r indent=indent1}
#install.packages("bootstrap")
library(bootstrap)
n <- nrow(law)

# use bootstrap method to estimate bias.
#sample estimate for n=15
theta.hat <- cor(law$LSAT, law$GPA)
#bootstrap estimate of bias
B <- 2000 #larger for estimating bias
theta.b <- numeric(B)
for (b in 1:B) {
  i <- sample(1:n, size = n, replace = TRUE)
  LSAT <- law$LSAT[i]
  GPA <- law$GPA[i]
  theta.b[b] <- cor(LSAT, GPA)
}
boot_bias <- mean(theta.b - theta.hat)

# use bootstrap method to estimate standard error.
#set up the bootstrap
B <- 200 #number of replicates
R <- numeric(B) #storage for replicates
#bootstrap estimate of standard error of R
for (b in 1:B) {
  #randomly select the indices
  i <- sample(1:n, size = n, replace = TRUE)
  LSAT <- law$LSAT[i] #i is a vector of indices
  GPA <- law$GPA[i]
  R[b] <- cor(LSAT, GPA)
}
boot_sd <- sd(R)

cat("Bootstrap estimate of bias:", boot_bias, "\n")
cat("Bootstrap estimate of standard error:", boot_sd, "\n")


# Calculate the jackknife estimates.
theta.jack <- numeric(n)
theta.hat <- cor(law$LSAT,law$GPA)
for (i in 1:n)
  theta.jack[i] <- cor(law$LSAT[-i],law$GPA[-i])

# Calculate the jackknife estimate of the bias.
jack_bias <- (n - 1) * (mean(theta.jack) - theta.hat)

# Calculate the jackknife estimate of the standard errorã€‚
jack_se <- sqrt((n - 1) * mean((theta.jack - mean(theta.jack))^2))

# Print the results
cat("Jackknife estimate of bias:", jack_bias, "\n")
cat("Jackknife estimate of standard error:", jack_se, "\n")

```
    <!-- solution end --> 
        
4. Refer to the air-conditioning data set _aircondit_ provided in the _boot_ package. The 12 observations are the times in hours between failures of air conditioning equipment:
$$
3,5,7,18,43,85,91,98,100,130,230,487 .
$$
Assume that the times between failures follow an exponential model $\operatorname{Exp}(\lambda)$. Obtain the MLE of the hazard rate $\lambda$ and use bootstrap to estimate the bias and standard error of the estimate. Let the number of replicates as $m=200$.

    <!-- solution begin -->
    __Solution__:
```{r indent=indent1}
library(boot)
set.seed(20)
m <- 200
n <- dim(aircondit)[1]

lambda_hat <- 1 / (mean(aircondit$hours)-aircondit$hours[1])

# use bootstrap method to estimate bias.
#bootstrap estimate of bias
lambda.boot <- numeric(m)
for (b in 1:m) {
  i <- sample(1:n, size = n, replace = TRUE)
  lambda.boot[b] <- 1 / mean(aircondit[i,1]-aircondit$hours[1])
}
boot_bias <- mean(lambda.boot - lambda_hat)

# use bootstrap method to estimate standard error.
boot_se <- sd(lambda.boot)

# Print the results
cat("MLE of lambda:", lambda_hat, "\n")
cat("Bias:", boot_bias, "\n")
cat("Standard error:", boot_se, "\n")
```
  
    <!-- solution end --> 


  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
